{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaperei/data-science/blob/main/lstm_amazon_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GibWyULarWgo"
      },
      "source": [
        "\n",
        "# Recurrent Neural Network - Long Short-Term Memory\n",
        "\n",
        "### Alison Augusto Miranda Pereira - UNIFESP - 2022\n",
        "\n",
        "## Descrição do problema\n",
        "\n",
        "O presente trabalho tem o objetivo de explorar uma das variações das redes `RNN` (*Recurrent Neural Network*) chamada `LSTM` (*Long Short-Term Memory*). A rede `LSTM` é capaz de armazenar longas sequências de informação, logo mostra-se mais eficiente em problemas onde esse ponto é um pré-requisito.\n",
        "\n",
        "O artigo ***Long Short-Term Memory: From Zero to Hero with PyTorch*** foi utilizado como referência para os códigos construídos - https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/.\n",
        "\n",
        "\n",
        "O problema escolhido consiste na análise de avaliações de usuários da Amazon. A base de dados completa encontra-se disponível no site do [Kaggle](https://www.kaggle.com/bittlingmayer/amazonreviews) e contém um total de 4 milhoes de avaliações, cada uma rotulado como `positiva` ou `negativa`.  Sendo assim, o objetiva da rede `LSTM` para esse problema será classificar cada uma das avalições como `positiva` ou `negativa`.\n",
        "\n",
        "O arquivo `lstm-amazon-reviews.ipynb` que você lendo também pode ser acessado através do meu ***Github*** pessoal https://github.com/aaperei/data-science/blob/main/lstm_amazon_reviews.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkg_4wKrWgx"
      },
      "source": [
        "## Pré-processamento dos dados\n",
        "\n",
        "\n",
        "Antes de aplicar os dados na rede `LSTM`, os dados passaram por uma etapa de pré-processamento. Os principais tratamentos aplicados aos dados foram:\n",
        "- Divisão dos dados em treinamento / validação / teste \n",
        "- Extração as classificações (rótulos) de cada avaliação - Positivo = 0, Negativo = 0\n",
        "- Substituição todas as URLs por \"\\<url>\" visto que maioria das URLs irá ter pouca ou nenhuma interferência na classificação da avaliação\n",
        "- Tokenização - Separação de cada avaliação em palavras individuais\n",
        "- Removeção palavras que aparecem uma única vez\n",
        "\n",
        "Vale ressaltar que não usaremos a base de dados completa nesse experimento por limitações de tempo e recursos de processamento disponiveis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWhOks_wrWgz",
        "outputId": "fc8a4400-540d-47a1-a14d-b340abc2f106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import bz2\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "D2mYKkwHrWg4"
      },
      "outputs": [],
      "source": [
        "train_file = bz2.BZ2File('/content/train.ft.txt.bz2')\n",
        "test_file = bz2.BZ2File('/content/test.ft.txt.bz2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-xMX1ifrWg5"
      },
      "outputs": [],
      "source": [
        "train_file = train_file.readlines()\n",
        "test_file = test_file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE545Gj0rWg6",
        "outputId": "55fbf942-2bd2-443f-dec3-8005b8db965a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training reviews: 3600000\n",
            "Number of test reviews: 400000\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training reviews: \" + str(len(train_file)))\n",
        "print(\"Number of test reviews: \" + str(len(test_file)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt-L-nuhrWg9"
      },
      "outputs": [],
      "source": [
        "num_train = 400000 #We're training on the first 400,000 reviews in the dataset\n",
        "num_test = 100000 #Using 100,000 reviews from test set\n",
        "\n",
        "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
        "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6Vlsbn0rWg-",
        "outputId": "dfe5eba0-85f6-4c66-b9b0-641f26d18d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train_file[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW4BoST9rWhB"
      },
      "outputs": [],
      "source": [
        "# Extracting labels from sentences\n",
        "\n",
        "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
        "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
        "\n",
        "    \n",
        "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
        "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n",
        "\n",
        "# Some simple cleaning of data\n",
        "\n",
        "for i in range(len(train_sentences)):\n",
        "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
        "\n",
        "# Modify URLs to <url>\n",
        "\n",
        "for i in range(len(train_sentences)):\n",
        "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
        "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
        "        \n",
        "for i in range(len(test_sentences)):\n",
        "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
        "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhEhtonorWhD"
      },
      "outputs": [],
      "source": [
        "del train_file, test_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu3Ho4mMrWhG",
        "outputId": "67d20de3-a128-400d-a8ca-7ce3e85de55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0% done\n",
            "5.0% done\n",
            "10.0% done\n",
            "15.0% done\n",
            "20.0% done\n",
            "25.0% done\n",
            "30.0% done\n",
            "35.0% done\n",
            "40.0% done\n",
            "45.0% done\n",
            "50.0% done\n",
            "55.0% done\n",
            "60.0% done\n",
            "65.0% done\n",
            "70.0% done\n",
            "75.0% done\n",
            "80.0% done\n",
            "85.0% done\n",
            "90.0% done\n",
            "95.0% done\n",
            "100% done\n"
          ]
        }
      ],
      "source": [
        "words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "    #The sentences will be stored as a list of words/tokens\n",
        "    train_sentences[i] = []\n",
        "    for word in nltk.word_tokenize(sentence): #Tokenizing the words\n",
        "        words.update([word.lower()]) #Converting all the words to lower case\n",
        "        train_sentences[i].append(word)\n",
        "    if i%20000 == 0:\n",
        "        print(str((i*100)/num_train) + \"% done\")\n",
        "print(\"100% done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrtG4V8BrWhJ"
      },
      "outputs": [],
      "source": [
        "# Removing the words that only appear once\n",
        "words = {k:v for k,v in words.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "words = sorted(words, key=words.get, reverse=True)\n",
        "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "words = ['_PAD','_UNK'] + words\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orKcm9pErWhK"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(train_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_sentences[i] = [word2idx[word] if word in word2idx else word2idx['_UNK'] for word in sentence]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    # For test sentences, we have to tokenize the sentences as well\n",
        "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in nltk.word_tokenize(sentence)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_5-EKVurWhM"
      },
      "outputs": [],
      "source": [
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
        "\n",
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW-ZXIndrWhN"
      },
      "outputs": [],
      "source": [
        "seq_len = 200 #The length that the sentences will be padded/shortened to\n",
        "\n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "test_sentences = pad_input(test_sentences, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ky5M-ENrWhN"
      },
      "outputs": [],
      "source": [
        "# Converting our labels into numpy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCjJoUBbrWhO",
        "outputId": "84428b1e-6499-41c3-c9aa-7ff6aaddeb2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,    40,   102,    13,    28,  1486,  3901,    59,\n",
              "          31,    10,     3,    40,  1782,    10,    85,  1638,     2,\n",
              "           5,    27,   917,     8,    11,   102,    16,   151,     6,\n",
              "           5,   141,    89,     9,     2,    69,     5,   122,    14,\n",
              "           7,    41,  1872,     9,   207,    60,   241,   107,     2,\n",
              "           7,   135,  1872,    47, 31468,    38,  3028,    14,     3,\n",
              "        2631,     2,    11,   102,    47, 17166,   158,     2,   945,\n",
              "          30,     1,     1,     6,   566,    47,  1288,     2,    31,\n",
              "          10,   158,    21,  2391,  4239,     2,    11,    12,     7,\n",
              "        3619, 17523,   102,    14,    28,    22,     2,   182,   101,\n",
              "         128,   146,     9,   236,    12,    47,   814,    60,     2,\n",
              "        2731,     5,   269,    11,     4,    72,   593,   447,     4,\n",
              "         573,     4,   405,     4,   153,     4,  1687,     4,  1265,\n",
              "        1826,   526,    31,   180,    33,    80,    18,    17,   856,\n",
              "          62,    32])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "test_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c85rsburrWhQ"
      },
      "outputs": [],
      "source": [
        "split_frac = 0.5\n",
        "split_id = int(split_frac * len(test_sentences))\n",
        "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
        "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtiKOOH5rWhS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
        "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmFPQFZWrWhV",
        "outputId": "84e93550-db7f-4599-c48b-2da7ddcf04be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2aXTQpSrWhW",
        "outputId": "7985e609-b9b2-43ea-dfd6-209a59c9c2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([400, 200]) torch.Size([400])\n"
          ]
        }
      ],
      "source": [
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print(sample_x.shape, sample_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8eGBLperWhX"
      },
      "source": [
        "## Definição e treinamento do modelo `LSTM`\n",
        "\n",
        "Nosso modelo `LSTM` é definido pela class `SentimentNet`. A camada final é uma camada totalmente conectada com função sigmoid para classificar se as avaliações são positivas ou negativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNQan9B3rWhX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0F2xbJdrWhY",
        "outputId": "5b65186e-529b-4a2d-9cfa-fe613a07039f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentNet(\n",
            "  (embedding): Embedding(144612, 400)\n",
            "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoyhyrrKrWhZ"
      },
      "outputs": [],
      "source": [
        "lr=0.005\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arahl6lGrWha",
        "outputId": "e59aca58-d7d7-4ba4-942d-daa9169d0062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2... Step: 1000... Loss: 0.173967... Val Loss: 0.176171\n",
            "Validation loss decreased (inf --> 0.176171).  Saving model ...\n",
            "Epoch: 2/2... Step: 2000... Loss: 0.156618... Val Loss: 0.187651\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "counter = 0\n",
        "print_every = 1000\n",
        "clip = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        h = tuple([e.data for e in h])\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if counter%print_every == 0:\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inp, lab in val_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                inp, lab = inp.to(device), lab.to(device)\n",
        "                out, val_h = model(inp, val_h)\n",
        "                val_loss = criterion(out.squeeze(), lab.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            if np.mean(val_losses) <= valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kef4yYyDrWhb",
        "outputId": "29491728-490d-4e80-b129-6ad580176c59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgYtyiicrWhc",
        "outputId": "d9bdd745-cae8-4a84-9204-e4596f6770e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.175\n",
            "Test accuracy: 93.392%\n"
          ]
        }
      ],
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze()) #rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpqI6_uDrWhc"
      },
      "source": [
        "## Análise final\n",
        "Como resultado final tivemos uma acurácia de **93.392%** durante a fase de testes. Tal resultado demonstra o poder de um modelo `LSTM` na resolução de problemas sequencias, onde temos a necessidade de armazenar muitos estados anteriores para a geração do resultado final.\n",
        "\n",
        "Note que essa acurácia foi alcançada sem utilizar toda a base de dados para treinamento e fazendo uso de camadas simples sem ajustes de hiperparâmetros."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "lstm-amazon-reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}